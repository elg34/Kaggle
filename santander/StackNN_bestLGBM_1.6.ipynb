{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 var: 256\n",
      "Corr>0.95: 139 Now:\n",
      "Corr Target <0.1: 4206\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "\n",
    "def drop_vars(df):\n",
    "    tmp=df.shape[1]\n",
    "    df = df[df.columns[[True]+list((df.var()!=0))]]\n",
    "    print('0 var:',tmp-df.shape[1])\n",
    "    \n",
    "    corr_matrix = df[df.columns[2:]].corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    tmp=df.shape[1]\n",
    "    df=df.drop(to_drop, axis=1)\n",
    "    print('Corr>0.95:',tmp-df.shape[1],'Now:',)\n",
    "    \n",
    "    corrs = dict()\n",
    "    for i in range(df.shape[1]-2):\n",
    "        corrs[df.columns[2+i]] = np.corrcoef(df['target'],df[df.columns[2+i]])[0,1]\n",
    "    s = [k for k in corrs if abs(corrs[k])<0.1]\n",
    "    tmp=df.shape[1]\n",
    "    df=df.drop(s, axis=1)\n",
    "    print('Corr Target <0.1:',tmp-df.shape[1])\n",
    "    \n",
    "    return df\n",
    "train = drop_vars(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[1]\tvalid_0's rmse: 1.73905\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's rmse: 1.73622\n",
      "[3]\tvalid_0's rmse: 1.7331\n",
      "[4]\tvalid_0's rmse: 1.73015\n",
      "[5]\tvalid_0's rmse: 1.72709\n",
      "[6]\tvalid_0's rmse: 1.72404\n",
      "[7]\tvalid_0's rmse: 1.72075\n",
      "[8]\tvalid_0's rmse: 1.71815\n",
      "[9]\tvalid_0's rmse: 1.71527\n",
      "[10]\tvalid_0's rmse: 1.71258\n",
      "[11]\tvalid_0's rmse: 1.71013\n",
      "[12]\tvalid_0's rmse: 1.70768\n",
      "[13]\tvalid_0's rmse: 1.70481\n",
      "[14]\tvalid_0's rmse: 1.70174\n",
      "[15]\tvalid_0's rmse: 1.69912\n",
      "[16]\tvalid_0's rmse: 1.69655\n",
      "[17]\tvalid_0's rmse: 1.69368\n",
      "[18]\tvalid_0's rmse: 1.69111\n",
      "[19]\tvalid_0's rmse: 1.68873\n",
      "[20]\tvalid_0's rmse: 1.68614\n",
      "[21]\tvalid_0's rmse: 1.68322\n",
      "[22]\tvalid_0's rmse: 1.68106\n",
      "[23]\tvalid_0's rmse: 1.67854\n",
      "[24]\tvalid_0's rmse: 1.67562\n",
      "[25]\tvalid_0's rmse: 1.67322\n",
      "[26]\tvalid_0's rmse: 1.67093\n",
      "[27]\tvalid_0's rmse: 1.66864\n",
      "[28]\tvalid_0's rmse: 1.66628\n",
      "[29]\tvalid_0's rmse: 1.66408\n",
      "[30]\tvalid_0's rmse: 1.66164\n",
      "[31]\tvalid_0's rmse: 1.65921\n",
      "[32]\tvalid_0's rmse: 1.6569\n",
      "[33]\tvalid_0's rmse: 1.65467\n",
      "[34]\tvalid_0's rmse: 1.6525\n",
      "[35]\tvalid_0's rmse: 1.65035\n",
      "[36]\tvalid_0's rmse: 1.64817\n",
      "[37]\tvalid_0's rmse: 1.64639\n",
      "[38]\tvalid_0's rmse: 1.64398\n",
      "[39]\tvalid_0's rmse: 1.64191\n",
      "[40]\tvalid_0's rmse: 1.63966\n",
      "[41]\tvalid_0's rmse: 1.63764\n",
      "[42]\tvalid_0's rmse: 1.63577\n",
      "[43]\tvalid_0's rmse: 1.6338\n",
      "[44]\tvalid_0's rmse: 1.63185\n",
      "[45]\tvalid_0's rmse: 1.62953\n",
      "[46]\tvalid_0's rmse: 1.62739\n",
      "[47]\tvalid_0's rmse: 1.62506\n",
      "[48]\tvalid_0's rmse: 1.62295\n",
      "[49]\tvalid_0's rmse: 1.62077\n",
      "[50]\tvalid_0's rmse: 1.61904\n",
      "[51]\tvalid_0's rmse: 1.61713\n",
      "[52]\tvalid_0's rmse: 1.61506\n",
      "[53]\tvalid_0's rmse: 1.61324\n",
      "[54]\tvalid_0's rmse: 1.61176\n",
      "[55]\tvalid_0's rmse: 1.61014\n",
      "[56]\tvalid_0's rmse: 1.60815\n",
      "[57]\tvalid_0's rmse: 1.60644\n",
      "[58]\tvalid_0's rmse: 1.60451\n",
      "[59]\tvalid_0's rmse: 1.60281\n",
      "[60]\tvalid_0's rmse: 1.60125\n",
      "[61]\tvalid_0's rmse: 1.59967\n",
      "[62]\tvalid_0's rmse: 1.59825\n",
      "[63]\tvalid_0's rmse: 1.59662\n",
      "[64]\tvalid_0's rmse: 1.59538\n",
      "[65]\tvalid_0's rmse: 1.59342\n",
      "[66]\tvalid_0's rmse: 1.59178\n",
      "[67]\tvalid_0's rmse: 1.59028\n",
      "[68]\tvalid_0's rmse: 1.58893\n",
      "[69]\tvalid_0's rmse: 1.58725\n",
      "[70]\tvalid_0's rmse: 1.58567\n",
      "[71]\tvalid_0's rmse: 1.58421\n",
      "[72]\tvalid_0's rmse: 1.58242\n",
      "[73]\tvalid_0's rmse: 1.58112\n",
      "[74]\tvalid_0's rmse: 1.57947\n",
      "[75]\tvalid_0's rmse: 1.57788\n",
      "[76]\tvalid_0's rmse: 1.57646\n",
      "[77]\tvalid_0's rmse: 1.57497\n",
      "[78]\tvalid_0's rmse: 1.57354\n",
      "[79]\tvalid_0's rmse: 1.57238\n",
      "[80]\tvalid_0's rmse: 1.57079\n",
      "[81]\tvalid_0's rmse: 1.5691\n",
      "[82]\tvalid_0's rmse: 1.56732\n",
      "[83]\tvalid_0's rmse: 1.56622\n",
      "[84]\tvalid_0's rmse: 1.56494\n",
      "[85]\tvalid_0's rmse: 1.56356\n",
      "[86]\tvalid_0's rmse: 1.56238\n",
      "[87]\tvalid_0's rmse: 1.56077\n",
      "[88]\tvalid_0's rmse: 1.55941\n",
      "[89]\tvalid_0's rmse: 1.55785\n",
      "[90]\tvalid_0's rmse: 1.55657\n",
      "[91]\tvalid_0's rmse: 1.55542\n",
      "[92]\tvalid_0's rmse: 1.55398\n",
      "[93]\tvalid_0's rmse: 1.55235\n",
      "[94]\tvalid_0's rmse: 1.55155\n",
      "[95]\tvalid_0's rmse: 1.55062\n",
      "[96]\tvalid_0's rmse: 1.54936\n",
      "[97]\tvalid_0's rmse: 1.5482\n",
      "[98]\tvalid_0's rmse: 1.54737\n",
      "[99]\tvalid_0's rmse: 1.54605\n",
      "[100]\tvalid_0's rmse: 1.54509\n",
      "[101]\tvalid_0's rmse: 1.54377\n",
      "[102]\tvalid_0's rmse: 1.54239\n",
      "[103]\tvalid_0's rmse: 1.5413\n",
      "[104]\tvalid_0's rmse: 1.54032\n",
      "[105]\tvalid_0's rmse: 1.53934\n",
      "[106]\tvalid_0's rmse: 1.53794\n",
      "[107]\tvalid_0's rmse: 1.53706\n",
      "[108]\tvalid_0's rmse: 1.53599\n",
      "[109]\tvalid_0's rmse: 1.53464\n",
      "[110]\tvalid_0's rmse: 1.53354\n",
      "[111]\tvalid_0's rmse: 1.53237\n",
      "[112]\tvalid_0's rmse: 1.53113\n",
      "[113]\tvalid_0's rmse: 1.52991\n",
      "[114]\tvalid_0's rmse: 1.52893\n",
      "[115]\tvalid_0's rmse: 1.52804\n",
      "[116]\tvalid_0's rmse: 1.5271\n",
      "[117]\tvalid_0's rmse: 1.52613\n",
      "[118]\tvalid_0's rmse: 1.5249\n",
      "[119]\tvalid_0's rmse: 1.52389\n",
      "[120]\tvalid_0's rmse: 1.52282\n",
      "[121]\tvalid_0's rmse: 1.52169\n",
      "[122]\tvalid_0's rmse: 1.52052\n",
      "[123]\tvalid_0's rmse: 1.51939\n",
      "[124]\tvalid_0's rmse: 1.51838\n",
      "[125]\tvalid_0's rmse: 1.51744\n",
      "[126]\tvalid_0's rmse: 1.51652\n",
      "[127]\tvalid_0's rmse: 1.51569\n",
      "[128]\tvalid_0's rmse: 1.5145\n",
      "[129]\tvalid_0's rmse: 1.51382\n",
      "[130]\tvalid_0's rmse: 1.51286\n",
      "[131]\tvalid_0's rmse: 1.51203\n",
      "[132]\tvalid_0's rmse: 1.51125\n",
      "[133]\tvalid_0's rmse: 1.51062\n",
      "[134]\tvalid_0's rmse: 1.50979\n",
      "[135]\tvalid_0's rmse: 1.50866\n",
      "[136]\tvalid_0's rmse: 1.50781\n",
      "[137]\tvalid_0's rmse: 1.50702\n",
      "[138]\tvalid_0's rmse: 1.50611\n",
      "[139]\tvalid_0's rmse: 1.50517\n",
      "[140]\tvalid_0's rmse: 1.50449\n",
      "[141]\tvalid_0's rmse: 1.5038\n",
      "[142]\tvalid_0's rmse: 1.50301\n",
      "[143]\tvalid_0's rmse: 1.50204\n",
      "[144]\tvalid_0's rmse: 1.50125\n",
      "[145]\tvalid_0's rmse: 1.50036\n",
      "[146]\tvalid_0's rmse: 1.49959\n",
      "[147]\tvalid_0's rmse: 1.49862\n",
      "[148]\tvalid_0's rmse: 1.49785\n",
      "[149]\tvalid_0's rmse: 1.49711\n",
      "[150]\tvalid_0's rmse: 1.49635\n",
      "[151]\tvalid_0's rmse: 1.49558\n",
      "[152]\tvalid_0's rmse: 1.49496\n",
      "[153]\tvalid_0's rmse: 1.49456\n",
      "[154]\tvalid_0's rmse: 1.49393\n",
      "[155]\tvalid_0's rmse: 1.49345\n",
      "[156]\tvalid_0's rmse: 1.49256\n",
      "[157]\tvalid_0's rmse: 1.4919\n",
      "[158]\tvalid_0's rmse: 1.49086\n",
      "[159]\tvalid_0's rmse: 1.49025\n",
      "[160]\tvalid_0's rmse: 1.48936\n",
      "[161]\tvalid_0's rmse: 1.48865\n",
      "[162]\tvalid_0's rmse: 1.48779\n",
      "[163]\tvalid_0's rmse: 1.48719\n",
      "[164]\tvalid_0's rmse: 1.48634\n",
      "[165]\tvalid_0's rmse: 1.48574\n",
      "[166]\tvalid_0's rmse: 1.48514\n",
      "[167]\tvalid_0's rmse: 1.48457\n",
      "[168]\tvalid_0's rmse: 1.4841\n",
      "[169]\tvalid_0's rmse: 1.48346\n",
      "[170]\tvalid_0's rmse: 1.48289\n",
      "[171]\tvalid_0's rmse: 1.48219\n",
      "[172]\tvalid_0's rmse: 1.48134\n",
      "[173]\tvalid_0's rmse: 1.48068\n",
      "[174]\tvalid_0's rmse: 1.47996\n",
      "[175]\tvalid_0's rmse: 1.47922\n",
      "[176]\tvalid_0's rmse: 1.47845\n",
      "[177]\tvalid_0's rmse: 1.47801\n",
      "[178]\tvalid_0's rmse: 1.47736\n",
      "[179]\tvalid_0's rmse: 1.47678\n",
      "[180]\tvalid_0's rmse: 1.47616\n",
      "[181]\tvalid_0's rmse: 1.4754\n",
      "[182]\tvalid_0's rmse: 1.47479\n",
      "[183]\tvalid_0's rmse: 1.47397\n",
      "[184]\tvalid_0's rmse: 1.47328\n",
      "[185]\tvalid_0's rmse: 1.47291\n",
      "[186]\tvalid_0's rmse: 1.4722\n",
      "[187]\tvalid_0's rmse: 1.47168\n",
      "[188]\tvalid_0's rmse: 1.47111\n",
      "[189]\tvalid_0's rmse: 1.47068\n",
      "[190]\tvalid_0's rmse: 1.47002\n",
      "[191]\tvalid_0's rmse: 1.46952\n",
      "[192]\tvalid_0's rmse: 1.46894\n",
      "[193]\tvalid_0's rmse: 1.46849\n",
      "[194]\tvalid_0's rmse: 1.46795\n",
      "[195]\tvalid_0's rmse: 1.46748\n",
      "[196]\tvalid_0's rmse: 1.46697\n",
      "[197]\tvalid_0's rmse: 1.46654\n",
      "[198]\tvalid_0's rmse: 1.46611\n",
      "[199]\tvalid_0's rmse: 1.46564\n",
      "[200]\tvalid_0's rmse: 1.465\n",
      "[201]\tvalid_0's rmse: 1.46468\n",
      "[202]\tvalid_0's rmse: 1.46418\n",
      "[203]\tvalid_0's rmse: 1.46354\n",
      "[204]\tvalid_0's rmse: 1.46303\n",
      "[205]\tvalid_0's rmse: 1.46274\n",
      "[206]\tvalid_0's rmse: 1.46254\n",
      "[207]\tvalid_0's rmse: 1.46228\n",
      "[208]\tvalid_0's rmse: 1.46178\n",
      "[209]\tvalid_0's rmse: 1.46131\n",
      "[210]\tvalid_0's rmse: 1.46072\n",
      "[211]\tvalid_0's rmse: 1.46035\n",
      "[212]\tvalid_0's rmse: 1.45988\n",
      "[213]\tvalid_0's rmse: 1.4594\n",
      "[214]\tvalid_0's rmse: 1.45886\n",
      "[215]\tvalid_0's rmse: 1.45832\n",
      "[216]\tvalid_0's rmse: 1.45779\n",
      "[217]\tvalid_0's rmse: 1.45749\n",
      "[218]\tvalid_0's rmse: 1.45695\n",
      "[219]\tvalid_0's rmse: 1.45652\n",
      "[220]\tvalid_0's rmse: 1.4558\n",
      "[221]\tvalid_0's rmse: 1.45532\n",
      "[222]\tvalid_0's rmse: 1.45496\n",
      "[223]\tvalid_0's rmse: 1.45461\n",
      "[224]\tvalid_0's rmse: 1.45426\n",
      "[225]\tvalid_0's rmse: 1.45369\n",
      "[226]\tvalid_0's rmse: 1.45318\n",
      "[227]\tvalid_0's rmse: 1.45269\n",
      "[228]\tvalid_0's rmse: 1.45204\n",
      "[229]\tvalid_0's rmse: 1.45144\n",
      "[230]\tvalid_0's rmse: 1.45085\n",
      "[231]\tvalid_0's rmse: 1.45051\n",
      "[232]\tvalid_0's rmse: 1.45012\n",
      "[233]\tvalid_0's rmse: 1.44963\n",
      "[234]\tvalid_0's rmse: 1.44921\n",
      "[235]\tvalid_0's rmse: 1.44878\n",
      "[236]\tvalid_0's rmse: 1.44864\n",
      "[237]\tvalid_0's rmse: 1.44836\n",
      "[238]\tvalid_0's rmse: 1.44807\n",
      "[239]\tvalid_0's rmse: 1.44764\n",
      "[240]\tvalid_0's rmse: 1.44739\n",
      "[241]\tvalid_0's rmse: 1.447\n",
      "[242]\tvalid_0's rmse: 1.44671\n",
      "[243]\tvalid_0's rmse: 1.44616\n",
      "[244]\tvalid_0's rmse: 1.44575\n",
      "[245]\tvalid_0's rmse: 1.44524\n",
      "[246]\tvalid_0's rmse: 1.44508\n",
      "[247]\tvalid_0's rmse: 1.44468\n",
      "[248]\tvalid_0's rmse: 1.44424\n",
      "[249]\tvalid_0's rmse: 1.44377\n",
      "[250]\tvalid_0's rmse: 1.44321\n",
      "[251]\tvalid_0's rmse: 1.44261\n",
      "[252]\tvalid_0's rmse: 1.4423\n",
      "[253]\tvalid_0's rmse: 1.442\n",
      "[254]\tvalid_0's rmse: 1.44177\n",
      "[255]\tvalid_0's rmse: 1.44125\n",
      "[256]\tvalid_0's rmse: 1.44102\n",
      "[257]\tvalid_0's rmse: 1.44071\n",
      "[258]\tvalid_0's rmse: 1.44052\n",
      "[259]\tvalid_0's rmse: 1.4401\n",
      "[260]\tvalid_0's rmse: 1.43999\n",
      "[261]\tvalid_0's rmse: 1.43977\n",
      "[262]\tvalid_0's rmse: 1.43965\n",
      "[263]\tvalid_0's rmse: 1.43949\n",
      "[264]\tvalid_0's rmse: 1.43911\n",
      "[265]\tvalid_0's rmse: 1.43881\n",
      "[266]\tvalid_0's rmse: 1.43867\n",
      "[267]\tvalid_0's rmse: 1.43833\n",
      "[268]\tvalid_0's rmse: 1.438\n",
      "[269]\tvalid_0's rmse: 1.43767\n",
      "[270]\tvalid_0's rmse: 1.43727\n",
      "[271]\tvalid_0's rmse: 1.43687\n",
      "[272]\tvalid_0's rmse: 1.43674\n",
      "[273]\tvalid_0's rmse: 1.43654\n",
      "[274]\tvalid_0's rmse: 1.43614\n",
      "[275]\tvalid_0's rmse: 1.43588\n",
      "[276]\tvalid_0's rmse: 1.43555\n",
      "[277]\tvalid_0's rmse: 1.43525\n",
      "[278]\tvalid_0's rmse: 1.43531\n",
      "[279]\tvalid_0's rmse: 1.43532\n",
      "[280]\tvalid_0's rmse: 1.43521\n",
      "[281]\tvalid_0's rmse: 1.4347\n",
      "[282]\tvalid_0's rmse: 1.43434\n",
      "[283]\tvalid_0's rmse: 1.43388\n",
      "[284]\tvalid_0's rmse: 1.43373\n",
      "[285]\tvalid_0's rmse: 1.43341\n",
      "[286]\tvalid_0's rmse: 1.43306\n",
      "[287]\tvalid_0's rmse: 1.43251\n",
      "[288]\tvalid_0's rmse: 1.43233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[289]\tvalid_0's rmse: 1.43217\n",
      "[290]\tvalid_0's rmse: 1.43191\n",
      "[291]\tvalid_0's rmse: 1.43167\n",
      "[292]\tvalid_0's rmse: 1.4315\n",
      "[293]\tvalid_0's rmse: 1.43156\n",
      "[294]\tvalid_0's rmse: 1.43136\n",
      "[295]\tvalid_0's rmse: 1.43105\n",
      "[296]\tvalid_0's rmse: 1.43092\n",
      "[297]\tvalid_0's rmse: 1.43081\n",
      "[298]\tvalid_0's rmse: 1.4306\n",
      "[299]\tvalid_0's rmse: 1.43025\n",
      "[300]\tvalid_0's rmse: 1.43022\n",
      "[301]\tvalid_0's rmse: 1.43001\n",
      "[302]\tvalid_0's rmse: 1.42978\n",
      "[303]\tvalid_0's rmse: 1.42938\n",
      "[304]\tvalid_0's rmse: 1.42922\n",
      "[305]\tvalid_0's rmse: 1.42901\n",
      "[306]\tvalid_0's rmse: 1.42899\n",
      "[307]\tvalid_0's rmse: 1.4288\n",
      "[308]\tvalid_0's rmse: 1.42851\n",
      "[309]\tvalid_0's rmse: 1.42823\n",
      "[310]\tvalid_0's rmse: 1.42813\n",
      "[311]\tvalid_0's rmse: 1.42789\n",
      "[312]\tvalid_0's rmse: 1.42782\n",
      "[313]\tvalid_0's rmse: 1.4276\n",
      "[314]\tvalid_0's rmse: 1.42737\n",
      "[315]\tvalid_0's rmse: 1.42722\n",
      "[316]\tvalid_0's rmse: 1.42682\n",
      "[317]\tvalid_0's rmse: 1.42665\n",
      "[318]\tvalid_0's rmse: 1.42641\n",
      "[319]\tvalid_0's rmse: 1.4263\n",
      "[320]\tvalid_0's rmse: 1.4263\n",
      "[321]\tvalid_0's rmse: 1.426\n",
      "[322]\tvalid_0's rmse: 1.42582\n",
      "[323]\tvalid_0's rmse: 1.42545\n",
      "[324]\tvalid_0's rmse: 1.42502\n",
      "[325]\tvalid_0's rmse: 1.42494\n",
      "[326]\tvalid_0's rmse: 1.42459\n",
      "[327]\tvalid_0's rmse: 1.42447\n",
      "[328]\tvalid_0's rmse: 1.42433\n",
      "[329]\tvalid_0's rmse: 1.42419\n",
      "[330]\tvalid_0's rmse: 1.42405\n",
      "[331]\tvalid_0's rmse: 1.42373\n",
      "[332]\tvalid_0's rmse: 1.4236\n",
      "[333]\tvalid_0's rmse: 1.42333\n",
      "[334]\tvalid_0's rmse: 1.42311\n",
      "[335]\tvalid_0's rmse: 1.42283\n",
      "[336]\tvalid_0's rmse: 1.4228\n",
      "[337]\tvalid_0's rmse: 1.42261\n",
      "[338]\tvalid_0's rmse: 1.42244\n",
      "[339]\tvalid_0's rmse: 1.42238\n",
      "[340]\tvalid_0's rmse: 1.42213\n",
      "[341]\tvalid_0's rmse: 1.42212\n",
      "[342]\tvalid_0's rmse: 1.42187\n",
      "[343]\tvalid_0's rmse: 1.42152\n",
      "[344]\tvalid_0's rmse: 1.42155\n",
      "[345]\tvalid_0's rmse: 1.4215\n",
      "[346]\tvalid_0's rmse: 1.42131\n",
      "[347]\tvalid_0's rmse: 1.42117\n",
      "[348]\tvalid_0's rmse: 1.42101\n",
      "[349]\tvalid_0's rmse: 1.42085\n",
      "[350]\tvalid_0's rmse: 1.42071\n",
      "[351]\tvalid_0's rmse: 1.42064\n",
      "[352]\tvalid_0's rmse: 1.42051\n",
      "[353]\tvalid_0's rmse: 1.42032\n",
      "[354]\tvalid_0's rmse: 1.42026\n",
      "[355]\tvalid_0's rmse: 1.42018\n",
      "[356]\tvalid_0's rmse: 1.42004\n",
      "[357]\tvalid_0's rmse: 1.41983\n",
      "[358]\tvalid_0's rmse: 1.41966\n",
      "[359]\tvalid_0's rmse: 1.41955\n",
      "[360]\tvalid_0's rmse: 1.4193\n",
      "[361]\tvalid_0's rmse: 1.41933\n",
      "[362]\tvalid_0's rmse: 1.41922\n",
      "[363]\tvalid_0's rmse: 1.41904\n",
      "[364]\tvalid_0's rmse: 1.41896\n",
      "[365]\tvalid_0's rmse: 1.41868\n",
      "[366]\tvalid_0's rmse: 1.41849\n",
      "[367]\tvalid_0's rmse: 1.41856\n",
      "[368]\tvalid_0's rmse: 1.41835\n",
      "[369]\tvalid_0's rmse: 1.41827\n",
      "[370]\tvalid_0's rmse: 1.4182\n",
      "[371]\tvalid_0's rmse: 1.41801\n",
      "[372]\tvalid_0's rmse: 1.41803\n",
      "[373]\tvalid_0's rmse: 1.41791\n",
      "[374]\tvalid_0's rmse: 1.41784\n",
      "[375]\tvalid_0's rmse: 1.41788\n",
      "[376]\tvalid_0's rmse: 1.41791\n",
      "[377]\tvalid_0's rmse: 1.41795\n",
      "[378]\tvalid_0's rmse: 1.41785\n",
      "[379]\tvalid_0's rmse: 1.4175\n",
      "[380]\tvalid_0's rmse: 1.41745\n",
      "[381]\tvalid_0's rmse: 1.41738\n",
      "[382]\tvalid_0's rmse: 1.41725\n",
      "[383]\tvalid_0's rmse: 1.4172\n",
      "[384]\tvalid_0's rmse: 1.41692\n",
      "[385]\tvalid_0's rmse: 1.41694\n",
      "[386]\tvalid_0's rmse: 1.41699\n",
      "[387]\tvalid_0's rmse: 1.417\n",
      "[388]\tvalid_0's rmse: 1.41695\n",
      "[389]\tvalid_0's rmse: 1.41686\n",
      "[390]\tvalid_0's rmse: 1.41677\n",
      "[391]\tvalid_0's rmse: 1.41648\n",
      "[392]\tvalid_0's rmse: 1.41624\n",
      "[393]\tvalid_0's rmse: 1.41617\n",
      "[394]\tvalid_0's rmse: 1.41591\n",
      "[395]\tvalid_0's rmse: 1.41586\n",
      "[396]\tvalid_0's rmse: 1.41573\n",
      "[397]\tvalid_0's rmse: 1.4157\n",
      "[398]\tvalid_0's rmse: 1.41571\n",
      "[399]\tvalid_0's rmse: 1.4156\n",
      "[400]\tvalid_0's rmse: 1.41547\n",
      "[401]\tvalid_0's rmse: 1.41528\n",
      "[402]\tvalid_0's rmse: 1.41524\n",
      "[403]\tvalid_0's rmse: 1.41516\n",
      "[404]\tvalid_0's rmse: 1.415\n",
      "[405]\tvalid_0's rmse: 1.41491\n",
      "[406]\tvalid_0's rmse: 1.41483\n",
      "[407]\tvalid_0's rmse: 1.41477\n",
      "[408]\tvalid_0's rmse: 1.41477\n",
      "[409]\tvalid_0's rmse: 1.41475\n",
      "[410]\tvalid_0's rmse: 1.41462\n",
      "[411]\tvalid_0's rmse: 1.41448\n",
      "[412]\tvalid_0's rmse: 1.41446\n",
      "[413]\tvalid_0's rmse: 1.41435\n",
      "[414]\tvalid_0's rmse: 1.41429\n",
      "[415]\tvalid_0's rmse: 1.41418\n",
      "[416]\tvalid_0's rmse: 1.41402\n",
      "[417]\tvalid_0's rmse: 1.41386\n",
      "[418]\tvalid_0's rmse: 1.41391\n",
      "[419]\tvalid_0's rmse: 1.41406\n",
      "[420]\tvalid_0's rmse: 1.41404\n",
      "[421]\tvalid_0's rmse: 1.41395\n",
      "[422]\tvalid_0's rmse: 1.41367\n",
      "[423]\tvalid_0's rmse: 1.41355\n",
      "[424]\tvalid_0's rmse: 1.41333\n",
      "[425]\tvalid_0's rmse: 1.4132\n",
      "[426]\tvalid_0's rmse: 1.41316\n",
      "[427]\tvalid_0's rmse: 1.41309\n",
      "[428]\tvalid_0's rmse: 1.41312\n",
      "[429]\tvalid_0's rmse: 1.41311\n",
      "[430]\tvalid_0's rmse: 1.41305\n",
      "[431]\tvalid_0's rmse: 1.41309\n",
      "[432]\tvalid_0's rmse: 1.41305\n",
      "[433]\tvalid_0's rmse: 1.41309\n",
      "[434]\tvalid_0's rmse: 1.41305\n",
      "[435]\tvalid_0's rmse: 1.41306\n",
      "[436]\tvalid_0's rmse: 1.41294\n",
      "[437]\tvalid_0's rmse: 1.41295\n",
      "[438]\tvalid_0's rmse: 1.41288\n",
      "[439]\tvalid_0's rmse: 1.41281\n",
      "[440]\tvalid_0's rmse: 1.41275\n",
      "[441]\tvalid_0's rmse: 1.41277\n",
      "[442]\tvalid_0's rmse: 1.41269\n",
      "[443]\tvalid_0's rmse: 1.41247\n",
      "[444]\tvalid_0's rmse: 1.4124\n",
      "[445]\tvalid_0's rmse: 1.41214\n",
      "[446]\tvalid_0's rmse: 1.41204\n",
      "[447]\tvalid_0's rmse: 1.41195\n",
      "[448]\tvalid_0's rmse: 1.41178\n",
      "[449]\tvalid_0's rmse: 1.41165\n",
      "[450]\tvalid_0's rmse: 1.41165\n",
      "[451]\tvalid_0's rmse: 1.41156\n",
      "[452]\tvalid_0's rmse: 1.41141\n",
      "[453]\tvalid_0's rmse: 1.41115\n",
      "[454]\tvalid_0's rmse: 1.41109\n",
      "[455]\tvalid_0's rmse: 1.41082\n",
      "[456]\tvalid_0's rmse: 1.41076\n",
      "[457]\tvalid_0's rmse: 1.41089\n",
      "[458]\tvalid_0's rmse: 1.4108\n",
      "[459]\tvalid_0's rmse: 1.41081\n",
      "[460]\tvalid_0's rmse: 1.41087\n",
      "[461]\tvalid_0's rmse: 1.41082\n",
      "Early stopping, best iteration is:\n",
      "[456]\tvalid_0's rmse: 1.41076\n",
      "Start predicting...\n",
      "1.4107575862807875\n"
     ]
    }
   ],
   "source": [
    "X=train[train.columns[2:]]\n",
    "y=np.log1p(train['target'])\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=203)\n",
    "\n",
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.transform(x_test)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return abs(np.sqrt(np.mean((y_pred - y_true)**2))) \n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_eval = lgb.Dataset(x_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'gamma',\n",
    "    'metric': {'rmse'},\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.008,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 4,\n",
    "    'max_depth': -1,\n",
    "    'reg_alpha': 0.3,\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'zero_as_missing': True,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(x_test, num_iteration=gbm.best_iteration)\n",
    "print(root_mean_squared_error(y_pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's rmse: 1.73905\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's rmse: 1.73622\n",
      "[3]\tvalid_0's rmse: 1.7331\n",
      "[4]\tvalid_0's rmse: 1.73015\n",
      "[5]\tvalid_0's rmse: 1.72709\n",
      "[6]\tvalid_0's rmse: 1.72404\n",
      "[7]\tvalid_0's rmse: 1.72075\n",
      "[8]\tvalid_0's rmse: 1.71815\n",
      "[9]\tvalid_0's rmse: 1.71527\n",
      "[10]\tvalid_0's rmse: 1.71258\n",
      "[11]\tvalid_0's rmse: 1.71013\n",
      "[12]\tvalid_0's rmse: 1.70768\n",
      "[13]\tvalid_0's rmse: 1.70481\n",
      "[14]\tvalid_0's rmse: 1.70174\n",
      "[15]\tvalid_0's rmse: 1.69912\n",
      "[16]\tvalid_0's rmse: 1.69655\n",
      "[17]\tvalid_0's rmse: 1.69368\n",
      "[18]\tvalid_0's rmse: 1.69111\n",
      "[19]\tvalid_0's rmse: 1.68873\n",
      "[20]\tvalid_0's rmse: 1.68614\n",
      "[21]\tvalid_0's rmse: 1.68322\n",
      "[22]\tvalid_0's rmse: 1.68106\n",
      "[23]\tvalid_0's rmse: 1.67854\n",
      "[24]\tvalid_0's rmse: 1.67562\n",
      "[25]\tvalid_0's rmse: 1.67322\n",
      "[26]\tvalid_0's rmse: 1.67093\n",
      "[27]\tvalid_0's rmse: 1.66864\n",
      "[28]\tvalid_0's rmse: 1.66628\n",
      "[29]\tvalid_0's rmse: 1.66408\n",
      "[30]\tvalid_0's rmse: 1.66164\n",
      "[31]\tvalid_0's rmse: 1.65921\n",
      "[32]\tvalid_0's rmse: 1.6569\n",
      "[33]\tvalid_0's rmse: 1.65467\n",
      "[34]\tvalid_0's rmse: 1.6525\n",
      "[35]\tvalid_0's rmse: 1.65035\n",
      "[36]\tvalid_0's rmse: 1.64817\n",
      "[37]\tvalid_0's rmse: 1.64639\n",
      "[38]\tvalid_0's rmse: 1.64398\n",
      "[39]\tvalid_0's rmse: 1.64191\n",
      "[40]\tvalid_0's rmse: 1.63966\n",
      "[41]\tvalid_0's rmse: 1.63764\n",
      "[42]\tvalid_0's rmse: 1.63577\n",
      "[43]\tvalid_0's rmse: 1.6338\n",
      "[44]\tvalid_0's rmse: 1.63185\n",
      "[45]\tvalid_0's rmse: 1.62953\n",
      "[46]\tvalid_0's rmse: 1.62739\n",
      "[47]\tvalid_0's rmse: 1.62506\n",
      "[48]\tvalid_0's rmse: 1.62295\n",
      "[49]\tvalid_0's rmse: 1.62077\n",
      "[50]\tvalid_0's rmse: 1.61904\n",
      "[51]\tvalid_0's rmse: 1.61713\n",
      "[52]\tvalid_0's rmse: 1.61506\n",
      "[53]\tvalid_0's rmse: 1.61324\n",
      "[54]\tvalid_0's rmse: 1.61176\n",
      "[55]\tvalid_0's rmse: 1.61014\n",
      "[56]\tvalid_0's rmse: 1.60815\n",
      "[57]\tvalid_0's rmse: 1.60644\n",
      "[58]\tvalid_0's rmse: 1.60451\n",
      "[59]\tvalid_0's rmse: 1.60281\n",
      "[60]\tvalid_0's rmse: 1.60125\n",
      "[61]\tvalid_0's rmse: 1.59967\n",
      "[62]\tvalid_0's rmse: 1.59825\n",
      "[63]\tvalid_0's rmse: 1.59662\n",
      "[64]\tvalid_0's rmse: 1.59538\n",
      "[65]\tvalid_0's rmse: 1.59342\n",
      "[66]\tvalid_0's rmse: 1.59178\n",
      "[67]\tvalid_0's rmse: 1.59028\n",
      "[68]\tvalid_0's rmse: 1.58893\n",
      "[69]\tvalid_0's rmse: 1.58725\n",
      "[70]\tvalid_0's rmse: 1.58567\n",
      "[71]\tvalid_0's rmse: 1.58421\n",
      "[72]\tvalid_0's rmse: 1.58242\n",
      "[73]\tvalid_0's rmse: 1.58112\n",
      "[74]\tvalid_0's rmse: 1.57947\n",
      "[75]\tvalid_0's rmse: 1.57788\n",
      "[76]\tvalid_0's rmse: 1.57646\n",
      "[77]\tvalid_0's rmse: 1.57497\n",
      "[78]\tvalid_0's rmse: 1.57354\n",
      "[79]\tvalid_0's rmse: 1.57238\n",
      "[80]\tvalid_0's rmse: 1.57079\n",
      "[81]\tvalid_0's rmse: 1.5691\n",
      "[82]\tvalid_0's rmse: 1.56732\n",
      "[83]\tvalid_0's rmse: 1.56622\n",
      "[84]\tvalid_0's rmse: 1.56494\n",
      "[85]\tvalid_0's rmse: 1.56356\n",
      "[86]\tvalid_0's rmse: 1.56238\n",
      "[87]\tvalid_0's rmse: 1.56077\n",
      "[88]\tvalid_0's rmse: 1.55941\n",
      "[89]\tvalid_0's rmse: 1.55785\n",
      "[90]\tvalid_0's rmse: 1.55657\n",
      "[91]\tvalid_0's rmse: 1.55542\n",
      "[92]\tvalid_0's rmse: 1.55398\n",
      "[93]\tvalid_0's rmse: 1.55235\n",
      "[94]\tvalid_0's rmse: 1.55155\n",
      "[95]\tvalid_0's rmse: 1.55062\n",
      "[96]\tvalid_0's rmse: 1.54936\n",
      "[97]\tvalid_0's rmse: 1.5482\n",
      "[98]\tvalid_0's rmse: 1.54737\n",
      "[99]\tvalid_0's rmse: 1.54605\n",
      "[100]\tvalid_0's rmse: 1.54509\n",
      "[101]\tvalid_0's rmse: 1.54377\n",
      "[102]\tvalid_0's rmse: 1.54239\n",
      "[103]\tvalid_0's rmse: 1.5413\n",
      "[104]\tvalid_0's rmse: 1.54032\n",
      "[105]\tvalid_0's rmse: 1.53934\n",
      "[106]\tvalid_0's rmse: 1.53794\n",
      "[107]\tvalid_0's rmse: 1.53706\n",
      "[108]\tvalid_0's rmse: 1.53599\n",
      "[109]\tvalid_0's rmse: 1.53464\n",
      "[110]\tvalid_0's rmse: 1.53354\n",
      "[111]\tvalid_0's rmse: 1.53237\n",
      "[112]\tvalid_0's rmse: 1.53113\n",
      "[113]\tvalid_0's rmse: 1.52991\n",
      "[114]\tvalid_0's rmse: 1.52893\n",
      "[115]\tvalid_0's rmse: 1.52804\n",
      "[116]\tvalid_0's rmse: 1.5271\n",
      "[117]\tvalid_0's rmse: 1.52613\n",
      "[118]\tvalid_0's rmse: 1.5249\n",
      "[119]\tvalid_0's rmse: 1.52389\n",
      "[120]\tvalid_0's rmse: 1.52282\n",
      "[121]\tvalid_0's rmse: 1.52169\n",
      "[122]\tvalid_0's rmse: 1.52052\n",
      "[123]\tvalid_0's rmse: 1.51939\n",
      "[124]\tvalid_0's rmse: 1.51838\n",
      "[125]\tvalid_0's rmse: 1.51744\n",
      "[126]\tvalid_0's rmse: 1.51652\n",
      "[127]\tvalid_0's rmse: 1.51569\n",
      "[128]\tvalid_0's rmse: 1.5145\n",
      "[129]\tvalid_0's rmse: 1.51382\n",
      "[130]\tvalid_0's rmse: 1.51286\n",
      "[131]\tvalid_0's rmse: 1.51203\n",
      "[132]\tvalid_0's rmse: 1.51125\n",
      "[133]\tvalid_0's rmse: 1.51062\n",
      "[134]\tvalid_0's rmse: 1.50979\n",
      "[135]\tvalid_0's rmse: 1.50866\n",
      "[136]\tvalid_0's rmse: 1.50781\n",
      "[137]\tvalid_0's rmse: 1.50702\n",
      "[138]\tvalid_0's rmse: 1.50611\n",
      "[139]\tvalid_0's rmse: 1.50517\n",
      "[140]\tvalid_0's rmse: 1.50449\n",
      "[141]\tvalid_0's rmse: 1.5038\n",
      "[142]\tvalid_0's rmse: 1.50301\n",
      "[143]\tvalid_0's rmse: 1.50204\n",
      "[144]\tvalid_0's rmse: 1.50125\n",
      "[145]\tvalid_0's rmse: 1.50036\n",
      "[146]\tvalid_0's rmse: 1.49959\n",
      "[147]\tvalid_0's rmse: 1.49862\n",
      "[148]\tvalid_0's rmse: 1.49785\n",
      "[149]\tvalid_0's rmse: 1.49711\n",
      "[150]\tvalid_0's rmse: 1.49635\n",
      "[151]\tvalid_0's rmse: 1.49558\n",
      "[152]\tvalid_0's rmse: 1.49496\n",
      "[153]\tvalid_0's rmse: 1.49456\n",
      "[154]\tvalid_0's rmse: 1.49393\n",
      "[155]\tvalid_0's rmse: 1.49345\n",
      "[156]\tvalid_0's rmse: 1.49256\n",
      "[157]\tvalid_0's rmse: 1.4919\n",
      "[158]\tvalid_0's rmse: 1.49086\n",
      "[159]\tvalid_0's rmse: 1.49025\n",
      "[160]\tvalid_0's rmse: 1.48936\n",
      "[161]\tvalid_0's rmse: 1.48865\n",
      "[162]\tvalid_0's rmse: 1.48779\n",
      "[163]\tvalid_0's rmse: 1.48719\n",
      "[164]\tvalid_0's rmse: 1.48634\n",
      "[165]\tvalid_0's rmse: 1.48574\n",
      "[166]\tvalid_0's rmse: 1.48514\n",
      "[167]\tvalid_0's rmse: 1.48457\n",
      "[168]\tvalid_0's rmse: 1.4841\n",
      "[169]\tvalid_0's rmse: 1.48346\n",
      "[170]\tvalid_0's rmse: 1.48289\n",
      "[171]\tvalid_0's rmse: 1.48219\n",
      "[172]\tvalid_0's rmse: 1.48134\n",
      "[173]\tvalid_0's rmse: 1.48068\n",
      "[174]\tvalid_0's rmse: 1.47996\n",
      "[175]\tvalid_0's rmse: 1.47922\n",
      "[176]\tvalid_0's rmse: 1.47845\n",
      "[177]\tvalid_0's rmse: 1.47801\n",
      "[178]\tvalid_0's rmse: 1.47736\n",
      "[179]\tvalid_0's rmse: 1.47678\n",
      "[180]\tvalid_0's rmse: 1.47616\n",
      "[181]\tvalid_0's rmse: 1.4754\n",
      "[182]\tvalid_0's rmse: 1.47479\n",
      "[183]\tvalid_0's rmse: 1.47397\n",
      "[184]\tvalid_0's rmse: 1.47328\n",
      "[185]\tvalid_0's rmse: 1.47291\n",
      "[186]\tvalid_0's rmse: 1.4722\n",
      "[187]\tvalid_0's rmse: 1.47168\n",
      "[188]\tvalid_0's rmse: 1.47111\n",
      "[189]\tvalid_0's rmse: 1.47068\n",
      "[190]\tvalid_0's rmse: 1.47002\n",
      "[191]\tvalid_0's rmse: 1.46952\n",
      "[192]\tvalid_0's rmse: 1.46894\n",
      "[193]\tvalid_0's rmse: 1.46849\n",
      "[194]\tvalid_0's rmse: 1.46795\n",
      "[195]\tvalid_0's rmse: 1.46748\n",
      "[196]\tvalid_0's rmse: 1.46697\n",
      "[197]\tvalid_0's rmse: 1.46654\n",
      "[198]\tvalid_0's rmse: 1.46611\n",
      "[199]\tvalid_0's rmse: 1.46564\n",
      "[200]\tvalid_0's rmse: 1.465\n",
      "[201]\tvalid_0's rmse: 1.46468\n",
      "[202]\tvalid_0's rmse: 1.46418\n",
      "[203]\tvalid_0's rmse: 1.46354\n",
      "[204]\tvalid_0's rmse: 1.46303\n",
      "[205]\tvalid_0's rmse: 1.46274\n",
      "[206]\tvalid_0's rmse: 1.46254\n",
      "[207]\tvalid_0's rmse: 1.46228\n",
      "[208]\tvalid_0's rmse: 1.46178\n",
      "[209]\tvalid_0's rmse: 1.46131\n",
      "[210]\tvalid_0's rmse: 1.46072\n",
      "[211]\tvalid_0's rmse: 1.46035\n",
      "[212]\tvalid_0's rmse: 1.45988\n",
      "[213]\tvalid_0's rmse: 1.4594\n",
      "[214]\tvalid_0's rmse: 1.45886\n",
      "[215]\tvalid_0's rmse: 1.45832\n",
      "[216]\tvalid_0's rmse: 1.45779\n",
      "[217]\tvalid_0's rmse: 1.45749\n",
      "[218]\tvalid_0's rmse: 1.45695\n",
      "[219]\tvalid_0's rmse: 1.45652\n",
      "[220]\tvalid_0's rmse: 1.4558\n",
      "[221]\tvalid_0's rmse: 1.45532\n",
      "[222]\tvalid_0's rmse: 1.45496\n",
      "[223]\tvalid_0's rmse: 1.45461\n",
      "[224]\tvalid_0's rmse: 1.45426\n",
      "[225]\tvalid_0's rmse: 1.45369\n",
      "[226]\tvalid_0's rmse: 1.45318\n",
      "[227]\tvalid_0's rmse: 1.45269\n",
      "[228]\tvalid_0's rmse: 1.45204\n",
      "[229]\tvalid_0's rmse: 1.45144\n",
      "[230]\tvalid_0's rmse: 1.45085\n",
      "[231]\tvalid_0's rmse: 1.45051\n",
      "[232]\tvalid_0's rmse: 1.45012\n",
      "[233]\tvalid_0's rmse: 1.44963\n",
      "[234]\tvalid_0's rmse: 1.44921\n",
      "[235]\tvalid_0's rmse: 1.44878\n",
      "[236]\tvalid_0's rmse: 1.44864\n",
      "[237]\tvalid_0's rmse: 1.44836\n",
      "[238]\tvalid_0's rmse: 1.44807\n",
      "[239]\tvalid_0's rmse: 1.44764\n",
      "[240]\tvalid_0's rmse: 1.44739\n",
      "[241]\tvalid_0's rmse: 1.447\n",
      "[242]\tvalid_0's rmse: 1.44671\n",
      "[243]\tvalid_0's rmse: 1.44616\n",
      "[244]\tvalid_0's rmse: 1.44575\n",
      "[245]\tvalid_0's rmse: 1.44524\n",
      "[246]\tvalid_0's rmse: 1.44508\n",
      "[247]\tvalid_0's rmse: 1.44468\n",
      "[248]\tvalid_0's rmse: 1.44424\n",
      "[249]\tvalid_0's rmse: 1.44377\n",
      "[250]\tvalid_0's rmse: 1.44321\n",
      "[251]\tvalid_0's rmse: 1.44261\n",
      "[252]\tvalid_0's rmse: 1.4423\n",
      "[253]\tvalid_0's rmse: 1.442\n",
      "[254]\tvalid_0's rmse: 1.44177\n",
      "[255]\tvalid_0's rmse: 1.44125\n",
      "[256]\tvalid_0's rmse: 1.44102\n",
      "[257]\tvalid_0's rmse: 1.44071\n",
      "[258]\tvalid_0's rmse: 1.44052\n",
      "[259]\tvalid_0's rmse: 1.4401\n",
      "[260]\tvalid_0's rmse: 1.43999\n",
      "[261]\tvalid_0's rmse: 1.43977\n",
      "[262]\tvalid_0's rmse: 1.43965\n",
      "[263]\tvalid_0's rmse: 1.43949\n",
      "[264]\tvalid_0's rmse: 1.43911\n",
      "[265]\tvalid_0's rmse: 1.43881\n",
      "[266]\tvalid_0's rmse: 1.43867\n",
      "[267]\tvalid_0's rmse: 1.43833\n",
      "[268]\tvalid_0's rmse: 1.438\n",
      "[269]\tvalid_0's rmse: 1.43767\n",
      "[270]\tvalid_0's rmse: 1.43727\n",
      "[271]\tvalid_0's rmse: 1.43687\n",
      "[272]\tvalid_0's rmse: 1.43674\n",
      "[273]\tvalid_0's rmse: 1.43654\n",
      "[274]\tvalid_0's rmse: 1.43614\n",
      "[275]\tvalid_0's rmse: 1.43588\n",
      "[276]\tvalid_0's rmse: 1.43555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[277]\tvalid_0's rmse: 1.43525\n",
      "[278]\tvalid_0's rmse: 1.43531\n",
      "[279]\tvalid_0's rmse: 1.43532\n",
      "[280]\tvalid_0's rmse: 1.43521\n",
      "[281]\tvalid_0's rmse: 1.4347\n",
      "[282]\tvalid_0's rmse: 1.43434\n",
      "[283]\tvalid_0's rmse: 1.43388\n",
      "[284]\tvalid_0's rmse: 1.43373\n",
      "[285]\tvalid_0's rmse: 1.43341\n",
      "[286]\tvalid_0's rmse: 1.43306\n",
      "[287]\tvalid_0's rmse: 1.43251\n",
      "[288]\tvalid_0's rmse: 1.43233\n",
      "[289]\tvalid_0's rmse: 1.43217\n",
      "[290]\tvalid_0's rmse: 1.43191\n",
      "[291]\tvalid_0's rmse: 1.43167\n",
      "[292]\tvalid_0's rmse: 1.4315\n",
      "[293]\tvalid_0's rmse: 1.43156\n",
      "[294]\tvalid_0's rmse: 1.43136\n",
      "[295]\tvalid_0's rmse: 1.43105\n",
      "[296]\tvalid_0's rmse: 1.43092\n",
      "[297]\tvalid_0's rmse: 1.43081\n",
      "[298]\tvalid_0's rmse: 1.4306\n",
      "[299]\tvalid_0's rmse: 1.43025\n",
      "[300]\tvalid_0's rmse: 1.43022\n",
      "[301]\tvalid_0's rmse: 1.43001\n",
      "[302]\tvalid_0's rmse: 1.42978\n",
      "[303]\tvalid_0's rmse: 1.42938\n",
      "[304]\tvalid_0's rmse: 1.42922\n",
      "[305]\tvalid_0's rmse: 1.42901\n",
      "[306]\tvalid_0's rmse: 1.42899\n",
      "[307]\tvalid_0's rmse: 1.4288\n",
      "[308]\tvalid_0's rmse: 1.42851\n",
      "[309]\tvalid_0's rmse: 1.42823\n",
      "[310]\tvalid_0's rmse: 1.42813\n",
      "[311]\tvalid_0's rmse: 1.42789\n",
      "[312]\tvalid_0's rmse: 1.42782\n",
      "[313]\tvalid_0's rmse: 1.4276\n",
      "[314]\tvalid_0's rmse: 1.42737\n",
      "[315]\tvalid_0's rmse: 1.42722\n",
      "[316]\tvalid_0's rmse: 1.42682\n",
      "[317]\tvalid_0's rmse: 1.42665\n",
      "[318]\tvalid_0's rmse: 1.42641\n",
      "[319]\tvalid_0's rmse: 1.4263\n",
      "[320]\tvalid_0's rmse: 1.4263\n",
      "[321]\tvalid_0's rmse: 1.426\n",
      "[322]\tvalid_0's rmse: 1.42582\n",
      "[323]\tvalid_0's rmse: 1.42545\n",
      "[324]\tvalid_0's rmse: 1.42502\n",
      "[325]\tvalid_0's rmse: 1.42494\n",
      "[326]\tvalid_0's rmse: 1.42459\n",
      "[327]\tvalid_0's rmse: 1.42447\n",
      "[328]\tvalid_0's rmse: 1.42433\n",
      "[329]\tvalid_0's rmse: 1.42419\n",
      "[330]\tvalid_0's rmse: 1.42405\n",
      "[331]\tvalid_0's rmse: 1.42373\n",
      "[332]\tvalid_0's rmse: 1.4236\n",
      "[333]\tvalid_0's rmse: 1.42333\n",
      "[334]\tvalid_0's rmse: 1.42311\n",
      "[335]\tvalid_0's rmse: 1.42283\n",
      "[336]\tvalid_0's rmse: 1.4228\n",
      "[337]\tvalid_0's rmse: 1.42261\n",
      "[338]\tvalid_0's rmse: 1.42244\n",
      "[339]\tvalid_0's rmse: 1.42238\n",
      "[340]\tvalid_0's rmse: 1.42213\n",
      "[341]\tvalid_0's rmse: 1.42212\n",
      "[342]\tvalid_0's rmse: 1.42187\n",
      "[343]\tvalid_0's rmse: 1.42152\n",
      "[344]\tvalid_0's rmse: 1.42155\n",
      "[345]\tvalid_0's rmse: 1.4215\n",
      "[346]\tvalid_0's rmse: 1.42131\n",
      "[347]\tvalid_0's rmse: 1.42117\n",
      "[348]\tvalid_0's rmse: 1.42101\n",
      "[349]\tvalid_0's rmse: 1.42085\n",
      "[350]\tvalid_0's rmse: 1.42071\n",
      "[351]\tvalid_0's rmse: 1.42064\n",
      "[352]\tvalid_0's rmse: 1.42051\n",
      "[353]\tvalid_0's rmse: 1.42032\n",
      "[354]\tvalid_0's rmse: 1.42026\n",
      "[355]\tvalid_0's rmse: 1.42018\n",
      "[356]\tvalid_0's rmse: 1.42004\n",
      "[357]\tvalid_0's rmse: 1.41983\n",
      "[358]\tvalid_0's rmse: 1.41966\n",
      "[359]\tvalid_0's rmse: 1.41955\n",
      "[360]\tvalid_0's rmse: 1.4193\n",
      "[361]\tvalid_0's rmse: 1.41933\n",
      "[362]\tvalid_0's rmse: 1.41922\n",
      "[363]\tvalid_0's rmse: 1.41904\n",
      "[364]\tvalid_0's rmse: 1.41896\n",
      "[365]\tvalid_0's rmse: 1.41868\n",
      "[366]\tvalid_0's rmse: 1.41849\n",
      "[367]\tvalid_0's rmse: 1.41856\n",
      "[368]\tvalid_0's rmse: 1.41835\n",
      "[369]\tvalid_0's rmse: 1.41827\n",
      "[370]\tvalid_0's rmse: 1.4182\n",
      "[371]\tvalid_0's rmse: 1.41801\n",
      "[372]\tvalid_0's rmse: 1.41803\n",
      "[373]\tvalid_0's rmse: 1.41791\n",
      "[374]\tvalid_0's rmse: 1.41784\n",
      "[375]\tvalid_0's rmse: 1.41788\n",
      "[376]\tvalid_0's rmse: 1.41791\n",
      "[377]\tvalid_0's rmse: 1.41795\n",
      "[378]\tvalid_0's rmse: 1.41785\n",
      "[379]\tvalid_0's rmse: 1.4175\n",
      "[380]\tvalid_0's rmse: 1.41745\n",
      "[381]\tvalid_0's rmse: 1.41738\n",
      "[382]\tvalid_0's rmse: 1.41725\n",
      "[383]\tvalid_0's rmse: 1.4172\n",
      "[384]\tvalid_0's rmse: 1.41692\n",
      "[385]\tvalid_0's rmse: 1.41694\n",
      "[386]\tvalid_0's rmse: 1.41699\n",
      "[387]\tvalid_0's rmse: 1.417\n",
      "[388]\tvalid_0's rmse: 1.41695\n",
      "[389]\tvalid_0's rmse: 1.41686\n",
      "[390]\tvalid_0's rmse: 1.41677\n",
      "[391]\tvalid_0's rmse: 1.41648\n",
      "[392]\tvalid_0's rmse: 1.41624\n",
      "[393]\tvalid_0's rmse: 1.41617\n",
      "[394]\tvalid_0's rmse: 1.41591\n",
      "[395]\tvalid_0's rmse: 1.41586\n",
      "[396]\tvalid_0's rmse: 1.41573\n",
      "[397]\tvalid_0's rmse: 1.4157\n",
      "[398]\tvalid_0's rmse: 1.41571\n",
      "[399]\tvalid_0's rmse: 1.4156\n",
      "[400]\tvalid_0's rmse: 1.41547\n",
      "[401]\tvalid_0's rmse: 1.41528\n",
      "[402]\tvalid_0's rmse: 1.41524\n",
      "[403]\tvalid_0's rmse: 1.41516\n",
      "[404]\tvalid_0's rmse: 1.415\n",
      "[405]\tvalid_0's rmse: 1.41491\n",
      "[406]\tvalid_0's rmse: 1.41483\n",
      "[407]\tvalid_0's rmse: 1.41477\n",
      "[408]\tvalid_0's rmse: 1.41477\n",
      "[409]\tvalid_0's rmse: 1.41475\n",
      "[410]\tvalid_0's rmse: 1.41462\n",
      "[411]\tvalid_0's rmse: 1.41448\n",
      "[412]\tvalid_0's rmse: 1.41446\n",
      "[413]\tvalid_0's rmse: 1.41435\n",
      "[414]\tvalid_0's rmse: 1.41429\n",
      "[415]\tvalid_0's rmse: 1.41418\n",
      "[416]\tvalid_0's rmse: 1.41402\n",
      "[417]\tvalid_0's rmse: 1.41386\n",
      "[418]\tvalid_0's rmse: 1.41391\n",
      "[419]\tvalid_0's rmse: 1.41406\n",
      "[420]\tvalid_0's rmse: 1.41404\n",
      "[421]\tvalid_0's rmse: 1.41395\n",
      "[422]\tvalid_0's rmse: 1.41367\n",
      "[423]\tvalid_0's rmse: 1.41355\n",
      "[424]\tvalid_0's rmse: 1.41333\n",
      "[425]\tvalid_0's rmse: 1.4132\n",
      "[426]\tvalid_0's rmse: 1.41316\n",
      "[427]\tvalid_0's rmse: 1.41309\n",
      "[428]\tvalid_0's rmse: 1.41312\n",
      "[429]\tvalid_0's rmse: 1.41311\n",
      "[430]\tvalid_0's rmse: 1.41305\n",
      "[431]\tvalid_0's rmse: 1.41309\n",
      "[432]\tvalid_0's rmse: 1.41305\n",
      "[433]\tvalid_0's rmse: 1.41309\n",
      "[434]\tvalid_0's rmse: 1.41305\n",
      "[435]\tvalid_0's rmse: 1.41306\n",
      "[436]\tvalid_0's rmse: 1.41294\n",
      "[437]\tvalid_0's rmse: 1.41295\n",
      "[438]\tvalid_0's rmse: 1.41288\n",
      "[439]\tvalid_0's rmse: 1.41281\n",
      "[440]\tvalid_0's rmse: 1.41275\n",
      "[441]\tvalid_0's rmse: 1.41277\n",
      "[442]\tvalid_0's rmse: 1.41269\n",
      "[443]\tvalid_0's rmse: 1.41247\n",
      "[444]\tvalid_0's rmse: 1.4124\n",
      "[445]\tvalid_0's rmse: 1.41214\n",
      "[446]\tvalid_0's rmse: 1.41204\n",
      "[447]\tvalid_0's rmse: 1.41195\n",
      "[448]\tvalid_0's rmse: 1.41178\n",
      "[449]\tvalid_0's rmse: 1.41165\n",
      "[450]\tvalid_0's rmse: 1.41165\n",
      "[451]\tvalid_0's rmse: 1.41156\n",
      "[452]\tvalid_0's rmse: 1.41141\n",
      "[453]\tvalid_0's rmse: 1.41115\n",
      "[454]\tvalid_0's rmse: 1.41109\n",
      "[455]\tvalid_0's rmse: 1.41082\n",
      "[456]\tvalid_0's rmse: 1.41076\n",
      "[457]\tvalid_0's rmse: 1.41089\n",
      "[458]\tvalid_0's rmse: 1.4108\n",
      "[459]\tvalid_0's rmse: 1.41081\n",
      "[460]\tvalid_0's rmse: 1.41087\n",
      "[461]\tvalid_0's rmse: 1.41082\n",
      "Early stopping, best iteration is:\n",
      "[456]\tvalid_0's rmse: 1.41076\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Train on 3567 samples, validate on 892 samples\n",
      "Epoch 1/1000\n",
      "3567/3567 [==============================] - 1s 175us/step - loss: 14.3621 - val_loss: 6.2891\n",
      "Epoch 2/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 9.0377 - val_loss: 9.6780\n",
      "Epoch 3/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 7.9240 - val_loss: 6.2698\n",
      "Epoch 4/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 7.1286 - val_loss: 4.3387\n",
      "Epoch 5/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 6.5743 - val_loss: 4.3675\n",
      "Epoch 6/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 5.7367 - val_loss: 4.2009\n",
      "Epoch 7/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 5.2028 - val_loss: 4.8556\n",
      "Epoch 8/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 4.7108 - val_loss: 4.3538\n",
      "Epoch 9/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 4.3085 - val_loss: 2.6555\n",
      "Epoch 10/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 3.9698 - val_loss: 2.3791\n",
      "Epoch 11/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 3.8620 - val_loss: 3.1412\n",
      "Epoch 12/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 3.2674 - val_loss: 3.1497\n",
      "Epoch 13/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 3.0106 - val_loss: 2.8781\n",
      "Epoch 14/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 2.6837 - val_loss: 2.0436\n",
      "Epoch 15/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 2.3632 - val_loss: 1.8511\n",
      "Epoch 16/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 2.0710 - val_loss: 1.5059\n",
      "Epoch 17/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.9630 - val_loss: 1.6411\n",
      "Epoch 18/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.7339 - val_loss: 1.4927\n",
      "Epoch 19/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.5758 - val_loss: 1.3169\n",
      "Epoch 20/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.4770 - val_loss: 1.4768\n",
      "Epoch 21/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3799 - val_loss: 1.3141\n",
      "Epoch 22/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3319 - val_loss: 1.3811\n",
      "Epoch 23/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.3355 - val_loss: 1.3069\n",
      "Epoch 24/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2802 - val_loss: 1.3278\n",
      "Epoch 25/1000\n",
      "3567/3567 [==============================] - 0s 14us/step - loss: 1.2677 - val_loss: 1.3140\n",
      "Epoch 26/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2459 - val_loss: 1.3104\n",
      "Epoch 27/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2370 - val_loss: 1.3122\n",
      "Epoch 28/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2405 - val_loss: 1.2978\n",
      "Epoch 29/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2502 - val_loss: 1.3135\n",
      "Epoch 30/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2422 - val_loss: 1.3042\n",
      "Epoch 31/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2450 - val_loss: 1.3566\n",
      "Epoch 32/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2342 - val_loss: 1.2996\n",
      "Epoch 33/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2405 - val_loss: 1.2968\n",
      "Epoch 34/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.2272 - val_loss: 1.3328\n",
      "Epoch 35/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2582 - val_loss: 1.3056\n",
      "Epoch 36/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2184 - val_loss: 1.3113\n",
      "Epoch 37/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2209 - val_loss: 1.2926\n",
      "Epoch 38/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.2173 - val_loss: 1.2919\n",
      "Epoch 39/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2144 - val_loss: 1.2926\n",
      "Epoch 40/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1850 - val_loss: 1.2838\n",
      "Epoch 41/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2171 - val_loss: 1.2647\n",
      "Epoch 42/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2039 - val_loss: 1.2755\n",
      "Epoch 43/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2011 - val_loss: 1.3221\n",
      "Epoch 44/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2085 - val_loss: 1.3073\n",
      "Epoch 45/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2002 - val_loss: 1.2871\n",
      "Epoch 46/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1834 - val_loss: 1.2792\n",
      "Epoch 47/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1907 - val_loss: 1.2556\n",
      "Epoch 48/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2136 - val_loss: 1.2890\n",
      "Epoch 49/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2057 - val_loss: 1.3065\n",
      "Epoch 50/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1919 - val_loss: 1.3381\n",
      "Epoch 51/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.2072 - val_loss: 1.2767\n",
      "Epoch 52/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1751 - val_loss: 1.2604\n",
      "Epoch 53/1000\n",
      "3567/3567 [==============================] - 0s 14us/step - loss: 1.1866 - val_loss: 1.2727\n",
      "Epoch 54/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2033 - val_loss: 1.2936\n",
      "Epoch 55/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1975 - val_loss: 1.2773\n",
      "Epoch 56/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1874 - val_loss: 1.2964\n",
      "Epoch 57/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.1880 - val_loss: 1.3154\n",
      "Epoch 58/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1776 - val_loss: 1.3074\n",
      "Epoch 59/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1746 - val_loss: 1.3196\n",
      "Epoch 60/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1751 - val_loss: 1.3057\n",
      "Epoch 61/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1705 - val_loss: 1.3001\n",
      "Epoch 62/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1480 - val_loss: 1.3121\n",
      "Epoch 63/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1538 - val_loss: 1.2919\n",
      "Epoch 64/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.1456 - val_loss: 1.2906\n",
      "Epoch 65/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.1558 - val_loss: 1.2881\n",
      "Epoch 66/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1485 - val_loss: 1.3032\n",
      "Epoch 67/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1292 - val_loss: 1.2858\n",
      "Epoch 68/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1393 - val_loss: 1.3022\n",
      "Epoch 69/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1492 - val_loss: 1.2923\n",
      "Epoch 70/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1221 - val_loss: 1.2828\n",
      "Epoch 71/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1316 - val_loss: 1.2865\n",
      "Epoch 72/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1273 - val_loss: 1.2801\n",
      "Epoch 73/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.1301 - val_loss: 1.2816\n",
      "Epoch 74/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.1269 - val_loss: 1.2836\n",
      "Epoch 75/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1319 - val_loss: 1.2820\n",
      "Epoch 76/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1249 - val_loss: 1.2829\n",
      "Epoch 77/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1291 - val_loss: 1.2868\n",
      "Epoch 78/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1312 - val_loss: 1.2810\n",
      "Epoch 79/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1251 - val_loss: 1.2829\n",
      "Epoch 80/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1153 - val_loss: 1.2824\n",
      "Epoch 81/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1375 - val_loss: 1.2840\n",
      "Epoch 82/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1233 - val_loss: 1.2866\n",
      "Epoch 83/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1202 - val_loss: 1.2852\n",
      "Epoch 84/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1097 - val_loss: 1.2843\n",
      "Epoch 85/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1279 - val_loss: 1.2826\n",
      "Epoch 86/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1310 - val_loss: 1.2808\n",
      "Epoch 87/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1341 - val_loss: 1.2809\n",
      "Epoch 88/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1278 - val_loss: 1.2827\n",
      "Epoch 89/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1056 - val_loss: 1.2827\n",
      "Epoch 90/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1080 - val_loss: 1.2812\n",
      "Epoch 91/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1228 - val_loss: 1.2815\n",
      "Epoch 92/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1362 - val_loss: 1.2815\n",
      "Epoch 93/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1169 - val_loss: 1.2813\n",
      "Epoch 94/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.1097 - val_loss: 1.2808\n",
      "Epoch 95/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1240 - val_loss: 1.2806\n",
      "Epoch 96/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1160 - val_loss: 1.2808\n",
      "Epoch 97/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.1323 - val_loss: 1.2815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf9bf9b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=train[train.columns[2:]]\n",
    "y=np.log1p(train['target'])\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=203)\n",
    "\n",
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.transform(x_test)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_eval = lgb.Dataset(x_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'gamma',\n",
    "    'metric': {'rmse'},\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.008,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 4,\n",
    "    'max_depth': -1,\n",
    "    'reg_alpha': 0.3,\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'zero_as_missing': True,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "\n",
    "scal=StandardScaler()\n",
    "x_train = np.c_[x_train,scal.fit_transform(gbm.predict(x_train, num_iteration=gbm.best_iteration).flatten().reshape(-1,1)).flatten()]\n",
    "x_test = np.c_[x_test,scal.transform(gbm.predict(x_test, num_iteration=gbm.best_iteration).flatten().reshape(-1,1)).flatten()]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=root_mean_squared_error,\n",
    "              optimizer=Adam(lr=0.1,decay=0.0001))\n",
    "\n",
    "\n",
    "checkp = ModelCheckpoint(filepath='weights.hdf5')\n",
    "lrred = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, cooldown=2, min_lr=0.000001)\n",
    "stp = EarlyStopping(monitor='val_loss', min_delta=0, patience=50)\n",
    "cbs = [checkp,lrred,stp]\n",
    "model.fit(x_train, y_train,\n",
    "        epochs=1000,\n",
    "        batch_size=400,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN+ElNet - Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4459/4459 [==============================] - 0s 52us/step - loss: 14.3501\n",
      "Epoch 2/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 8.7264\n",
      "Epoch 3/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 7.2545\n",
      "Epoch 4/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 6.7290\n",
      "Epoch 5/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 5.7271\n",
      "Epoch 6/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 5.1289\n",
      "Epoch 7/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 4.6125\n",
      "Epoch 8/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 4.0757\n",
      "Epoch 9/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 3.4672\n",
      "Epoch 10/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 2.9047\n",
      "Epoch 11/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 2.5308\n",
      "Epoch 12/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 2.2900\n",
      "Epoch 13/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.8869\n",
      "Epoch 14/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.6244\n",
      "Epoch 15/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.4635\n",
      "Epoch 16/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.3984\n",
      "Epoch 17/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.3309\n",
      "Epoch 18/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.2841\n",
      "Epoch 19/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.2880\n",
      "Epoch 20/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.2723\n",
      "Epoch 21/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.2521\n",
      "Epoch 22/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.2067\n",
      "Epoch 23/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1722\n",
      "Epoch 24/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1339\n",
      "Epoch 25/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.1634\n",
      "Epoch 26/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1425\n",
      "Epoch 27/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1432\n",
      "Epoch 28/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1329\n",
      "Epoch 29/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.1421\n",
      "Epoch 30/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1400\n",
      "Epoch 31/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.1637\n",
      "Epoch 32/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1681\n",
      "Epoch 33/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1543\n",
      "Epoch 34/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1423\n",
      "Epoch 35/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1281\n",
      "Epoch 36/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1329\n",
      "Epoch 37/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1151\n",
      "Epoch 38/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1148\n",
      "Epoch 39/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1229\n",
      "Epoch 40/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1206\n",
      "Epoch 41/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.1416\n",
      "Epoch 42/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1660\n",
      "Epoch 43/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.1380\n",
      "Epoch 44/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1193\n",
      "Epoch 45/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1391\n",
      "Epoch 46/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1262\n",
      "Epoch 47/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1258\n",
      "Epoch 48/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1113\n",
      "Epoch 49/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1518\n",
      "Epoch 50/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.1308\n",
      "Epoch 51/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1248\n",
      "Epoch 52/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1455\n",
      "Epoch 53/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1197\n",
      "Epoch 54/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.1041\n",
      "Epoch 55/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0953\n",
      "Epoch 56/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.1406\n",
      "Epoch 57/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.1090\n",
      "Epoch 58/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0923\n",
      "Epoch 59/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0953\n",
      "Epoch 60/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0944\n",
      "Epoch 61/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0988\n",
      "Epoch 62/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0956\n",
      "Epoch 63/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0856\n",
      "Epoch 64/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.1153\n",
      "Epoch 65/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0982\n",
      "Epoch 66/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1047\n",
      "Epoch 67/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1379\n",
      "Epoch 68/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1592\n",
      "Epoch 69/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1707\n",
      "Epoch 70/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.1510\n",
      "Epoch 71/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.1387\n",
      "Epoch 72/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.1159\n",
      "Epoch 73/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.1141\n",
      "Epoch 74/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.1150\n",
      "Epoch 75/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0901\n",
      "Epoch 76/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0834\n",
      "Epoch 77/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0936\n",
      "Epoch 78/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0898\n",
      "Epoch 79/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0680\n",
      "Epoch 80/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0612\n",
      "Epoch 81/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0731\n",
      "Epoch 82/1000\n",
      "4459/4459 [==============================] - 0s 17us/step - loss: 1.0634\n",
      "Epoch 83/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0664\n",
      "Epoch 84/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0510\n",
      "Epoch 85/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0521\n",
      "Epoch 86/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0520\n",
      "Epoch 87/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0706\n",
      "Epoch 88/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0523\n",
      "Epoch 89/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0578\n",
      "Epoch 90/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0665\n",
      "Epoch 91/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0506\n",
      "Epoch 92/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0578\n",
      "Epoch 93/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0383\n",
      "Epoch 94/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0744\n",
      "Epoch 95/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0767\n",
      "Epoch 96/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0457\n",
      "Epoch 97/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0524\n",
      "Epoch 98/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0368\n",
      "Epoch 99/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0347\n",
      "Epoch 100/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0482\n",
      "Epoch 101/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0522\n",
      "Epoch 102/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0337\n",
      "Epoch 103/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0292\n",
      "Epoch 104/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0578\n",
      "Epoch 105/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0409\n",
      "Epoch 106/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0261\n",
      "Epoch 107/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0287\n",
      "Epoch 108/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0358\n",
      "Epoch 109/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0291\n",
      "Epoch 110/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0608\n",
      "Epoch 111/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0657\n",
      "Epoch 112/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0349\n",
      "Epoch 113/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0267\n",
      "Epoch 114/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0305\n",
      "Epoch 115/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0389\n",
      "Epoch 116/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0376\n",
      "Epoch 117/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0265\n",
      "Epoch 118/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0091\n",
      "Epoch 119/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0233\n",
      "Epoch 120/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0180\n",
      "Epoch 121/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0215\n",
      "Epoch 122/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0253\n",
      "Epoch 123/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0194\n",
      "Epoch 124/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0097\n",
      "Epoch 125/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0207\n",
      "Epoch 126/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0204\n",
      "Epoch 127/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0021\n",
      "Epoch 128/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0087\n",
      "Epoch 129/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 0.9950\n",
      "Epoch 130/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0027\n",
      "Epoch 131/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0125\n",
      "Epoch 132/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0069\n",
      "Epoch 133/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0180\n",
      "Epoch 134/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0133\n",
      "Epoch 135/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0107\n",
      "Epoch 136/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0029\n",
      "Epoch 137/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0087\n",
      "Epoch 138/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0103\n",
      "Epoch 139/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9971\n",
      "Epoch 140/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0069\n",
      "Epoch 141/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9988\n",
      "Epoch 142/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0045\n",
      "Epoch 143/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0150\n",
      "Epoch 144/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 0.9899\n",
      "Epoch 145/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 0.9960\n",
      "Epoch 146/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9926\n",
      "Epoch 147/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0076\n",
      "Epoch 148/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0222\n",
      "Epoch 149/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9977\n",
      "Epoch 150/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9927\n",
      "Epoch 151/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0075\n",
      "Epoch 152/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0044\n",
      "Epoch 153/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0145\n",
      "Epoch 154/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0003\n",
      "Epoch 155/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 0.9996\n",
      "Epoch 156/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0012\n",
      "Epoch 157/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9990\n",
      "Epoch 158/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9987\n",
      "Epoch 159/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0127\n",
      "Epoch 160/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0001\n",
      "Epoch 161/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0143\n",
      "Epoch 162/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0065\n",
      "Epoch 163/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9958\n",
      "Epoch 164/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0010\n",
      "Epoch 165/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0135\n",
      "Epoch 166/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0121\n",
      "Epoch 167/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 0.9889\n",
      "Epoch 168/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0021\n",
      "Epoch 169/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0055\n",
      "Epoch 170/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0048\n",
      "Epoch 171/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0068\n",
      "Epoch 172/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0053\n",
      "Epoch 173/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0165\n",
      "Epoch 174/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 0.9978\n",
      "Epoch 175/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0141\n",
      "Epoch 176/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0040\n",
      "Epoch 177/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9975\n",
      "Epoch 178/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0131\n",
      "Epoch 179/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0024\n",
      "Epoch 180/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0221\n",
      "Epoch 181/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9960\n",
      "Epoch 182/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0124\n",
      "Epoch 183/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9946\n",
      "Epoch 184/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0150\n",
      "Epoch 185/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9953\n",
      "Epoch 186/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0092\n",
      "Epoch 187/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0113\n",
      "Epoch 188/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9976\n",
      "Epoch 189/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9982\n",
      "Epoch 190/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0040\n",
      "Epoch 191/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0059\n",
      "Epoch 192/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9960\n",
      "Epoch 193/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 0.9960\n",
      "Epoch 194/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0019\n",
      "Epoch 195/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 0.9971\n",
      "Epoch 196/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0054\n",
      "Epoch 197/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 0.9961\n",
      "Epoch 198/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0113\n",
      "Epoch 199/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0156\n",
      "Epoch 200/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0244\n",
      "Epoch 201/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 0.9991\n",
      "Epoch 202/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9967\n",
      "Epoch 203/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 0.9966\n",
      "Epoch 204/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 0.9991\n",
      "Epoch 205/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0085\n",
      "Epoch 206/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 1.0068\n",
      "Epoch 207/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9964\n",
      "Epoch 208/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 0.9959\n",
      "Epoch 209/1000\n",
      "4459/4459 [==============================] - 0s 13us/step - loss: 0.9999\n",
      "Epoch 210/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0004\n",
      "Epoch 211/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0046\n",
      "Epoch 212/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 0.9967\n",
      "Epoch 213/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0009\n",
      "Epoch 214/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0144\n",
      "Epoch 215/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.0107\n",
      "Epoch 216/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0047\n",
      "Epoch 217/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.0063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf5b3e10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=train[train.columns[2:]]\n",
    "y_train=np.log1p(train['target'])\n",
    "\n",
    "x_test=test[x_train.columns]\n",
    "\n",
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.transform(x_test)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return abs(K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)))\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_eval = lgb.Dataset(x_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'gamma',\n",
    "    'metric': {'rmse'},\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.008,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 4,\n",
    "    'max_depth': -1,\n",
    "    'reg_alpha': 0.3,\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'zero_as_missing': True,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000)\n",
    "\n",
    "\n",
    "scal=StandardScaler()\n",
    "x_train = np.c_[x_train,scal.fit_transform(gbm.predict(x_train, num_iteration=gbm.best_iteration).flatten().reshape(-1,1)).flatten()]\n",
    "x_test = np.c_[x_test,scal.transform(gbm.predict(x_test, num_iteration=gbm.best_iteration).flatten().reshape(-1,1)).flatten()]\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=root_mean_squared_error,\n",
    "              optimizer=Adam(lr=0.1,decay=0.0001))\n",
    "\n",
    "\n",
    "checkp = ModelCheckpoint(filepath='weights.hdf5')\n",
    "lrred = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=10, cooldown=2, min_lr=0.000001)\n",
    "stp = EarlyStopping(monitor='loss', min_delta=0, patience=50)\n",
    "cbs = [checkp,lrred,stp]\n",
    "model.fit(x_train, y_train,\n",
    "        epochs=1000,\n",
    "        batch_size=400,\n",
    "        callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49342/49342 [==============================] - 1s 14us/step\n",
      "          ID      target\n",
      "0  000137c73  1728998.25\n",
      "1  00021489f  2572478.50\n",
      "2  0004d7953  3053373.50\n",
      "3  00056a333  3818850.75\n",
      "4  00056d8eb  2572478.50\n"
     ]
    }
   ],
   "source": [
    "predictions=pd.DataFrame({'ID':test['ID'],'target':np.expm1(model.predict(x_test, verbose=1).flatten())})\n",
    "print(predictions.head())\n",
    "predictions.to_csv('pred_nn_lgbm.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
