{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 var: 256\n",
      "Corr>0.95: 139 Now:\n",
      "Corr Target <0.1: 4206\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "\n",
    "def drop_vars(df):\n",
    "    tmp=df.shape[1]\n",
    "    df = df[df.columns[[True]+list((df.var()!=0))]]\n",
    "    print('0 var:',tmp-df.shape[1])\n",
    "    \n",
    "    corr_matrix = df[df.columns[2:]].corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    tmp=df.shape[1]\n",
    "    df=df.drop(to_drop, axis=1)\n",
    "    print('Corr>0.95:',tmp-df.shape[1],'Now:',)\n",
    "    \n",
    "    corrs = dict()\n",
    "    for i in range(df.shape[1]-2):\n",
    "        corrs[df.columns[2+i]] = np.corrcoef(df['target'],df[df.columns[2+i]])[0,1]\n",
    "    s = [k for k in corrs if abs(corrs[k])<0.1]\n",
    "    tmp=df.shape[1]\n",
    "    df=df.drop(s, axis=1)\n",
    "    print('Corr Target <0.1:',tmp-df.shape[1])\n",
    "    \n",
    "    return df\n",
    "train = drop_vars(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4813580179455834\n",
      "1.7418805964740947\n"
     ]
    }
   ],
   "source": [
    "X=train[train.columns[2:]]\n",
    "y=np.log1p(train['target'])\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=203)\n",
    "\n",
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.transform(x_test)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return abs(np.sqrt(np.mean((y_pred - y_true)**2))) \n",
    "\n",
    "clf1 = GradientBoostingRegressor()\n",
    "clf1.fit(x_train, y_train)\n",
    "clf2 = ElasticNet(random_state=23,alpha=5)\n",
    "clf2.fit(x_train, y_train)\n",
    "scal=StandardScaler()\n",
    "\n",
    "y_true,y_pred = y_test,clf1.predict(x_test) \n",
    "print(root_mean_squared_error(y_pred, y_true))\n",
    "\n",
    "y_true,y_pred = y_test,clf2.predict(x_test) \n",
    "print(root_mean_squared_error(y_pred, y_true))\n",
    "\n",
    "x_train = np.c_[x_train,scal.fit_transform(clf1.predict(x_train).flatten().reshape(-1,1)).flatten(),scal.fit_transform(clf2.predict(x_train).flatten().reshape(-1,1)).flatten()]\n",
    "x_test = np.c_[x_test,scal.transform(clf1.predict(x_test).flatten().reshape(-1,1)).flatten(),scal.transform(clf2.predict(x_test).flatten().reshape(-1,1)).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Train on 3567 samples, validate on 892 samples\n",
      "Epoch 1/1000\n",
      "3567/3567 [==============================] - 0s 71us/step - loss: 17.8847 - val_loss: 6.9188\n",
      "Epoch 2/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 9.4651 - val_loss: 6.3885\n",
      "Epoch 3/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 8.5719 - val_loss: 6.0183\n",
      "Epoch 4/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 7.7191 - val_loss: 6.9903\n",
      "Epoch 5/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 7.0431 - val_loss: 6.4274\n",
      "Epoch 6/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 6.4993 - val_loss: 6.6714\n",
      "Epoch 7/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 6.0701 - val_loss: 6.3325\n",
      "Epoch 8/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 5.6869 - val_loss: 4.2829\n",
      "Epoch 9/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 4.7474 - val_loss: 4.3295\n",
      "Epoch 10/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 4.4098 - val_loss: 4.0553\n",
      "Epoch 11/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 3.9251 - val_loss: 3.8747\n",
      "Epoch 12/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 3.6510 - val_loss: 3.2537\n",
      "Epoch 13/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 3.1650 - val_loss: 2.5821\n",
      "Epoch 14/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 2.8237 - val_loss: 2.2956\n",
      "Epoch 15/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 2.4555 - val_loss: 2.3528\n",
      "Epoch 16/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 2.1989 - val_loss: 1.8418\n",
      "Epoch 17/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 2.0319 - val_loss: 2.1724\n",
      "Epoch 18/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.8447 - val_loss: 1.7055\n",
      "Epoch 19/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.5959 - val_loss: 1.5290\n",
      "Epoch 20/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.4587 - val_loss: 1.4451\n",
      "Epoch 21/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.4030 - val_loss: 1.4067\n",
      "Epoch 22/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.3722 - val_loss: 1.3895\n",
      "Epoch 23/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.3634 - val_loss: 1.3766\n",
      "Epoch 24/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.3668 - val_loss: 1.3677\n",
      "Epoch 25/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3455 - val_loss: 1.3741\n",
      "Epoch 26/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3436 - val_loss: 1.3749\n",
      "Epoch 27/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.3377 - val_loss: 1.3744\n",
      "Epoch 28/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3374 - val_loss: 1.3719\n",
      "Epoch 29/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3329 - val_loss: 1.3598\n",
      "Epoch 30/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3403 - val_loss: 1.3756\n",
      "Epoch 31/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3315 - val_loss: 1.3802\n",
      "Epoch 32/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3413 - val_loss: 1.3727\n",
      "Epoch 33/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3253 - val_loss: 1.3710\n",
      "Epoch 34/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3166 - val_loss: 1.3719\n",
      "Epoch 35/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3256 - val_loss: 1.3857\n",
      "Epoch 36/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3239 - val_loss: 1.3765\n",
      "Epoch 37/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3182 - val_loss: 1.3958\n",
      "Epoch 38/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3257 - val_loss: 1.3781\n",
      "Epoch 39/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3166 - val_loss: 1.3936\n",
      "Epoch 40/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3122 - val_loss: 1.3699\n",
      "Epoch 41/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3098 - val_loss: 1.3809\n",
      "Epoch 42/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.3059 - val_loss: 1.3720\n",
      "Epoch 43/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2998 - val_loss: 1.3741\n",
      "Epoch 44/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2872 - val_loss: 1.3701\n",
      "Epoch 45/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2962 - val_loss: 1.3765\n",
      "Epoch 46/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.2885 - val_loss: 1.3655\n",
      "Epoch 47/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2791 - val_loss: 1.3711\n",
      "Epoch 48/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2879 - val_loss: 1.3690\n",
      "Epoch 49/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2815 - val_loss: 1.3602\n",
      "Epoch 50/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2861 - val_loss: 1.3651\n",
      "Epoch 51/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2961 - val_loss: 1.3653\n",
      "Epoch 52/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.2818 - val_loss: 1.3674\n",
      "Epoch 53/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2935 - val_loss: 1.3696\n",
      "Epoch 54/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2627 - val_loss: 1.3648\n",
      "Epoch 55/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2791 - val_loss: 1.3651\n",
      "Epoch 56/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2894 - val_loss: 1.3689\n",
      "Epoch 57/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2910 - val_loss: 1.3689\n",
      "Epoch 58/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2800 - val_loss: 1.3655\n",
      "Epoch 59/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.2766 - val_loss: 1.3644\n",
      "Epoch 60/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2855 - val_loss: 1.3629\n",
      "Epoch 61/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2834 - val_loss: 1.3667\n",
      "Epoch 62/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2860 - val_loss: 1.3659\n",
      "Epoch 63/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2888 - val_loss: 1.3651\n",
      "Epoch 64/1000\n",
      "3567/3567 [==============================] - 0s 16us/step - loss: 1.2792 - val_loss: 1.3636\n",
      "Epoch 65/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2724 - val_loss: 1.3626\n",
      "Epoch 66/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2665 - val_loss: 1.3629\n",
      "Epoch 67/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2766 - val_loss: 1.3642\n",
      "Epoch 68/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2727 - val_loss: 1.3648\n",
      "Epoch 69/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2862 - val_loss: 1.3665\n",
      "Epoch 70/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2834 - val_loss: 1.3672\n",
      "Epoch 71/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2743 - val_loss: 1.3661\n",
      "Epoch 72/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2801 - val_loss: 1.3647\n",
      "Epoch 73/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2846 - val_loss: 1.3646\n",
      "Epoch 74/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2829 - val_loss: 1.3646\n",
      "Epoch 75/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2777 - val_loss: 1.3644\n",
      "Epoch 76/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2698 - val_loss: 1.3642\n",
      "Epoch 77/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2825 - val_loss: 1.3640\n",
      "Epoch 78/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2808 - val_loss: 1.3640\n",
      "Epoch 79/1000\n",
      "3567/3567 [==============================] - 0s 15us/step - loss: 1.2685 - val_loss: 1.3639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xfa15e48>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=train[train.columns[2:]]\n",
    "y=np.log1p(train['target'])\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=203)\n",
    "\n",
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.transform(x_test)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "clf1 = GradientBoostingRegressor()\n",
    "clf1.fit(x_train, y_train)\n",
    "clf2 = ElasticNet(random_state=23,alpha=5)\n",
    "clf2.fit(x_train, y_train)\n",
    "\n",
    "scal=StandardScaler()\n",
    "x_train = np.c_[x_train,scal.fit_transform(clf1.predict(x_train).flatten().reshape(-1,1)).flatten(),scal.fit_transform(clf2.predict(x_train).flatten().reshape(-1,1)).flatten()]\n",
    "x_test = np.c_[x_test,scal.transform(clf1.predict(x_test).flatten().reshape(-1,1)).flatten(),scal.transform(clf2.predict(x_test).flatten().reshape(-1,1)).flatten()]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=root_mean_squared_error,\n",
    "              optimizer=Adam(lr=0.1,decay=0.0001))\n",
    "\n",
    "\n",
    "checkp = ModelCheckpoint(filepath='weights.hdf5')\n",
    "lrred = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, cooldown=2, min_lr=0.000001)\n",
    "stp = EarlyStopping(monitor='val_loss', min_delta=0, patience=50)\n",
    "cbs = [checkp,lrred,stp]\n",
    "model.fit(x_train, y_train,\n",
    "        epochs=1000,\n",
    "        batch_size=400,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN+ElNet - Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4459/4459 [==============================] - 0s 58us/step - loss: 14.9585\n",
      "Epoch 2/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 9.1811\n",
      "Epoch 3/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 8.0042\n",
      "Epoch 4/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 6.5724\n",
      "Epoch 5/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 5.6546\n",
      "Epoch 6/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 5.0037\n",
      "Epoch 7/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 4.5723\n",
      "Epoch 8/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 4.2494\n",
      "Epoch 9/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 3.6154\n",
      "Epoch 10/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 3.2197\n",
      "Epoch 11/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 2.7131\n",
      "Epoch 12/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 2.4022\n",
      "Epoch 13/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 2.0943\n",
      "Epoch 14/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.8754\n",
      "Epoch 15/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.6125\n",
      "Epoch 16/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.4542\n",
      "Epoch 17/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.4443\n",
      "Epoch 18/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.4182\n",
      "Epoch 19/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.3669\n",
      "Epoch 20/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.3454\n",
      "Epoch 21/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.3453\n",
      "Epoch 22/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.3153\n",
      "Epoch 23/1000\n",
      "4459/4459 [==============================] - 0s 15us/step - loss: 1.3221\n",
      "Epoch 24/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.3017\n",
      "Epoch 25/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.2795\n",
      "Epoch 26/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.2778\n",
      "Epoch 27/1000\n",
      "4459/4459 [==============================] - 0s 14us/step - loss: 1.2543\n",
      "Epoch 28/1000\n",
      " 400/4459 [=>............................] - ETA: 0s - loss: 1.2355"
     ]
    }
   ],
   "source": [
    "x_train=train[train.columns[2:]]\n",
    "y_train=np.log1p(train['target'])\n",
    "\n",
    "x_test=test[x_train.columns]\n",
    "\n",
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.transform(x_test)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return abs(K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)))\n",
    "\n",
    "clf = GradientBoostingRegressor()\n",
    "clf.fit(x_train, y_train)\n",
    "scal=StandardScaler()\n",
    "x_train = np.c_[x_train,scal.fit_transform(clf.predict(x_train).flatten().reshape(-1,1)).flatten()]\n",
    "x_test = np.c_[x_test,scal.transform(clf.predict(x_test).flatten().reshape(-1,1)).flatten()]\n",
    "\n",
    "clf = ElasticNet(random_state=23,alpha=5)\n",
    "clf.fit(x_train, y_train)\n",
    "scal=StandardScaler()\n",
    "x_train = np.c_[x_train,scal.fit_transform(clf.predict(x_train).flatten().reshape(-1,1)).flatten()]\n",
    "x_test = np.c_[x_test,scal.transform(clf.predict(x_test).flatten().reshape(-1,1)).flatten()]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=root_mean_squared_error,\n",
    "              optimizer=Adam(lr=0.1,decay=0.0001))\n",
    "\n",
    "\n",
    "checkp = ModelCheckpoint(filepath='weights.hdf5')\n",
    "lrred = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=10, cooldown=2, min_lr=0.000001)\n",
    "stp = EarlyStopping(monitor='loss', min_delta=0, patience=50)\n",
    "cbs = [checkp,lrred,stp]\n",
    "model.fit(x_train, y_train,\n",
    "        epochs=1000,\n",
    "        batch_size=400,\n",
    "        callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49342/49342 [==============================] - 1s 16us/step\n",
      "          ID      target\n",
      "0  000137c73  1752246.00\n",
      "1  00021489f  2149595.00\n",
      "2  0004d7953  2535024.25\n",
      "3  00056a333  2129491.00\n",
      "4  00056d8eb  2149595.00\n"
     ]
    }
   ],
   "source": [
    "predictions=pd.DataFrame({'ID':test['ID'],'target':np.expm1(model.predict(x_test, verbose=1).flatten())})\n",
    "print(predictions.head())\n",
    "predictions.to_csv('pred_boost.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
