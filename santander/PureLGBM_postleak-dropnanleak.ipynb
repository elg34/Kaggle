{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/johnfarrell/breaking-lb-fresh-start-with-lag-selection/output\n",
    "train_base=pd.read_csv('train.csv')\n",
    "test_base=pd.read_csv('test.csv')\n",
    "\n",
    "train_leak=pd.read_csv('train_leak.csv')\n",
    "test_leak=pd.read_csv('test_leak.csv')\n",
    "\n",
    "trainleak = train_leak['compiled_leak'].values\n",
    "trainlogleak = np.log1p(train_leak['compiled_leak'].values)\n",
    "testleak = test_leak['compiled_leak'].values\n",
    "testlogleak = np.log1p(test_leak['compiled_leak'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_vars(df):\n",
    "    tmp=df.shape[1]\n",
    "    df = df[df.columns[[True]+list((df.var()!=0))]]\n",
    "    print('0 var:',tmp-df.shape[1])\n",
    "    \n",
    "    corr_matrix = df[df.columns[2:]].corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    tmp=df.shape[1]\n",
    "    df=df.drop(to_drop, axis=1)\n",
    "    print('Corr>0.95:',tmp-df.shape[1])\n",
    "    \n",
    "    corrs = dict()\n",
    "    for i in range(df.shape[1]-2):\n",
    "        corrs[df.columns[2+i]] = np.corrcoef(df['target'],df[df.columns[2+i]])[0,1]\n",
    "    s = [k for k in corrs if abs(corrs[k])<0.1]\n",
    "    tmp=df.shape[1]\n",
    "    df=df.drop(s, axis=1)\n",
    "    print('Corr Target <0.1:',tmp-df.shape[1])\n",
    "    \n",
    "    return df\n",
    "tmp=drop_vars(train_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tmp.copy()\n",
    "test = test_base.copy()\n",
    "\n",
    "train['leak'] = trainleak\n",
    "train['log_leak'] = trainlogleak\n",
    "test['leak'] = testleak\n",
    "test['log_leak'] = testlogleak\n",
    "\n",
    "train = train[np.isfinite(train['leak'])]\n",
    "test = test[np.isfinite(test['leak'])]\n",
    "\n",
    "#https://www.kaggle.com/ogrellier/feature-scoring-vs-zeros/notebook\n",
    "features = [f for f in train if f not in ['ID', 'leak', 'log_leak', 'target']]\n",
    "train.replace(0, np.nan, inplace=True)\n",
    "train['log_of_mean'] = np.log1p(train[features].replace(0, np.nan).mean(axis=1))\n",
    "train['mean_of_log'] = np.log1p(train[features]).replace(0, np.nan).mean(axis=1)\n",
    "train['log_of_median'] = np.log1p(train[features].replace(0, np.nan).median(axis=1))\n",
    "train['nnans'] = train[features].isnull().sum(axis=1)\n",
    "train['sum'] = np.log1p(train[features].sum(axis=1))\n",
    "train['std'] = train[features].std(axis=1)\n",
    "train['kurtosis'] = train[features].kurtosis(axis=1)\n",
    "\n",
    "test.replace(0, np.nan, inplace=True)\n",
    "test['log_of_mean'] = np.log1p(test[features].replace(0, np.nan).mean(axis=1))\n",
    "test['mean_of_log'] = np.log1p(test[features]).replace(0, np.nan).mean(axis=1)\n",
    "test['log_of_median'] = np.log1p(test[features].replace(0, np.nan).median(axis=1))\n",
    "test['nnans'] = test[features].isnull().sum(axis=1)\n",
    "test['sum'] = np.log1p(test[features].sum(axis=1))\n",
    "test['std'] = test[features].std(axis=1)\n",
    "test['kurtosis'] = test[features].kurtosis(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.replace(np.nan,0, inplace=True)\n",
    "test.replace(np.nan,0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train[train.columns[2:]]\n",
    "y=np.log1p(train['target'])\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=203)\n",
    "\n",
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.transform(x_test)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return abs(np.sqrt(np.mean((y_pred - y_true)**2))) \n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_eval = lgb.Dataset(x_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'gamma',\n",
    "    'metric': {'rmse'},\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 4,\n",
    "    'max_depth': -1,\n",
    "    'reg_alpha': 0.3,\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'zero_as_missing': True,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(x_test, num_iteration=gbm.best_iteration)\n",
    "# eval\n",
    "print('The rmse of prediction is:', root_mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From one of the successful kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate:  0.015\n",
      "[200]\tcv_agg's rmse: 0.73944 + 0.0362683\n",
      "[400]\tcv_agg's rmse: 0.717533 + 0.0390627\n",
      "Optimal Round: 498\n",
      "Optimal Score: 0.7159211643356415 + 0.03957621054593416\n",
      "###########################################################################################\n",
      "Learning Rate:  0.01\n",
      "[200]\tcv_agg's rmse: 0.789428 + 0.0344841\n",
      "[400]\tcv_agg's rmse: 0.721741 + 0.0397164\n",
      "[600]\tcv_agg's rmse: 0.714193 + 0.0402347\n",
      "[800]\tcv_agg's rmse: 0.713488 + 0.0402673\n",
      "Optimal Round: 739\n",
      "Optimal Score: 0.713152119703968 + 0.040588511405705394\n",
      "###########################################################################################\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "learning_rates = [0.015,0.01]\n",
    "for param in learning_rates:\n",
    "    print(\"Learning Rate: \", param)\n",
    "    modelstart= time.time()\n",
    "    params[\"learning_rate\"] = param\n",
    "    # Find Optimal Parameters / Boosting Rounds\n",
    "    lgb_cv = lgb.cv(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round=1000,\n",
    "        stratified=False,\n",
    "        nfold = 5,\n",
    "        verbose_eval=200,\n",
    "        seed = 23,\n",
    "        early_stopping_rounds=75)\n",
    "\n",
    "    optimal_rounds = np.argmin(lgb_cv['rmse-mean'])\n",
    "    best_cv_score = min(lgb_cv['rmse-mean'])\n",
    "\n",
    "    print(\"Optimal Round: {}\\nOptimal Score: {} + {}\".format(\n",
    "        optimal_rounds,best_cv_score,lgb_cv['rmse-stdv'][optimal_rounds]))\n",
    "    print(\"###########################################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train=train[train.columns[2:]]\n",
    "y_train=np.log1p(train['target'])\n",
    "\n",
    "x_test=test[x_train.columns]\n",
    "\n",
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.transform(x_test)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return abs(K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)))\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'gamma',\n",
    "    'metric': {'rmse'},\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 4,\n",
    "    'max_depth': -1,\n",
    "    'reg_alpha': 0.3,\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'zero_as_missing': True,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10000)\n",
    "\n",
    "y_pred = gbm.predict(x_test, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID        target\n",
      "0  000137c73  1.576021e+06\n",
      "1  00021489f  2.404527e+06\n",
      "2  0004d7953  2.769994e+06\n",
      "3  00056a333  8.087247e+06\n",
      "4  00056d8eb  2.404527e+06\n"
     ]
    }
   ],
   "source": [
    "predictions=pd.DataFrame({'ID':test['ID'],'target':np.expm1(y_pred)})\n",
    "print(predictions.head())\n",
    "predictions.to_csv('pred_lgbmleak.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
